{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program extracting first column \n",
    "import xlrd\n",
    "\n",
    "loc = (\"./utils/Modified_Bioinfer_label.xlsx\")\n",
    "\n",
    "wb = xlrd.open_workbook(loc)\n",
    "sheet = wb.sheet_by_index(0) \n",
    "sheet.cell_value(0, 0) \n",
    "PPI = []\n",
    "\n",
    "for i in range(1, sheet.nrows): \n",
    "    PPI.append([sheet.cell_value(i, 4), sheet.cell_value(i, 5), sheet.cell_value(i, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_PPI = [list(x) for x in set(tuple(x) for x in PPI)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_PPI = PPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract PDB Files from Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdb import *\n",
    "import Bio\n",
    "from Bio.PDB import PDBList\n",
    "pdbl = PDBList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_gene = {}\n",
    "def gene_to_pdb(pdb_id):\n",
    "    found_pdbs = Query(pdb_id).search()\n",
    "    print (found_pdbs)\n",
    "    temp= []\n",
    "    for j in found_pdbs:\n",
    "        temp.append(int(describe_pdb(j)['nr_residues']))\n",
    "    final_pdb_id = found_pdbs[temp.index(max(temp))]\n",
    "    protein_gene[pdb_id] = final_pdb_id\n",
    "    print (final_pdb_id)\n",
    "    pdbl.retrieve_pdb_file(final_pdb_id,pdir='PDB', file_format='pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_protein = []\n",
    "for proteins in unique_PPI:\n",
    "    unique_protein.append(proteins[0])\n",
    "    unique_protein.append(proteins[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_protein = list(set(unique_protein))\n",
    "# print(len(unique_protein), \"instances of unique proteins found from\", len(unique_PPI), \"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_PPI) // 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ppi in unique_protein:\n",
    "#     gene_to_pdb(ppi)\n",
    "#     if len(protein_gene) % 50 == 0:\n",
    "#         with open('protein_gene.txt', 'w') as f:\n",
    "#             f.write(str(protein_gene))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "with open('./utils/protein_pdb.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "    protein_pdb = ast.literal_eval(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([\"N\", \"C\", \"O\", \"H\", \"S\", \"N1+\"])\n",
    "    \n",
    "def get_ms_feature_vector(gene):\n",
    "    if(gene is None):\n",
    "        return([])\n",
    "    temp = []\n",
    "    try:\n",
    "        with open('./PDB/' + gene +'.pdb', 'r') as f1:\n",
    "            lines = f1.readlines()\n",
    "            for line in lines:\n",
    "                split_ = line.split()\n",
    "                if split_[0] == 'ATOM':\n",
    "                    temp.extend([float(split_[6]), float(split_[7]), float(split_[8])])  \n",
    "                    temp.extend(label_encoder.transform([split_[-1]]))#.toarray().squeeze().tolist())\n",
    "    except:\n",
    "        pass\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for x in tqdm(range(0, 1)):\n",
    "    nodes = []\n",
    "    labels = []\n",
    "    feature_vector = {}\n",
    "    for i in range(x * len(unique_PPI) // 25, (x + 1) * len(unique_PPI) // 25):\n",
    "        proteins = unique_PPI[i]\n",
    "        temp = []\n",
    "        try:\n",
    "            gene_1 = protein_pdb[proteins[0]]\n",
    "        except:\n",
    "            gene_1 = None\n",
    "        try:\n",
    "            gene_2 = protein_pdb[proteins[1]]\n",
    "        except:\n",
    "            gene_2 = None\n",
    "        if proteins[0] not in feature_vector:\n",
    "            feature_vector[gene_1] = get_ms_feature_vector(gene_1)\n",
    "        temp.extend(feature_vector[gene_1])\n",
    "        temp.extend([0] * (431520 - len(temp)))\n",
    "        if proteins[1] not in feature_vector:\n",
    "            feature_vector[gene_2] = get_ms_feature_vector(gene_2)\n",
    "        temp.extend(feature_vector[gene_2])\n",
    "        temp.extend([0] * (863040 - len(temp)))\n",
    "        nodes.append(temp)\n",
    "        labels.append(float(proteins[2]))\n",
    "    encoding = []\n",
    "    nodes_tens = torch.tensor(nodes)\n",
    "    nodes_tens = pre_process(nodes_tens)\n",
    "    for i in tqdm(range(0, len(nodes_tens), 5)):\n",
    "        encoding.extend(encoder.forward(nodes_tens[i:i + 5]).tolist())\n",
    "    del nodes_tens\n",
    "    with open(\"./utils/node_enc{}.csv\".format(x), \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([len(node) for node in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nodes))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./utils/unique_ppi.txt', 'w') as filehandle:\n",
    "#     for listitem in unique_PPI:\n",
    "#         filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# with open(\"./utils/node.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(nodes)\n",
    "\n",
    "# with open(\"./utils/label.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = []\n",
    "encoding = []\n",
    "# labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./utils/node.csv', 'r') as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file)\n",
    "#     line_count = 0\n",
    "#     for row in csv_reader:\n",
    "#         nodes.extend([float(val) for val in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./utils/label.csv', 'r') as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file)\n",
    "#     line_count = 0\n",
    "#     for row in csv_reader:\n",
    "#         labels.extend([float(val) for val in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nodes))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(tens):\n",
    "    tens = torch.reshape(tens, (tens.shape[0], -1, 4))\n",
    "    tens = torch.unsqueeze(tens, 0)\n",
    "    tens = torch.transpose(tens, 0, 1)\n",
    "    return(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "#             MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "#             MaxPool2d(kernel_size=2, stride=2),\n",
    "            Conv2d(4, 4, kernel_size=5, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(431516, 1185)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_tens = torch.tensor(nodes)\n",
    "nodes_tens = pre_process(nodes_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(nodes_tens), 5)):\n",
    "    encoding.extend(encoder.forward(nodes_tens[i:i + 5]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nodes_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./utils/node_enc{}.csv\".format(a), \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(encoding)\n",
    "a=a+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from CSV to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "nodes_enc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, 25):        \n",
    "    with open(\"./utils/node_enc{}.csv\".format(x), \"r\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if(len(row) != 0):\n",
    "                nodes_enc.append([float(val) for val in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for node in nodes_enc:\n",
    "    temp = \"\\t\".join([str(elem) for elem in node])\n",
    "    lines.append(temp + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./utils/node_MS', 'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define empty list\n",
    "unique_PPI = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('./utils/unique_ppi.txt', 'r') as filehandle:\n",
    "    filecontents = filehandle.readlines()\n",
    "\n",
    "    for line in filecontents:\n",
    "        temp = []\n",
    "        # remove linebreak which is the last character of the string\n",
    "        line = line[:-1]\n",
    "        line = line.strip('[').strip(']').split(\", \")\n",
    "        temp = [item.replace(\"'\", \"\") for item in line]\n",
    "        # add item to the list\n",
    "        unique_PPI.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_PPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "with open('./utils/protein_gene.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "    protein_gene = ast.literal_eval(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for i in tqdm_notebook(range(0, len(unique_PPI))):\n",
    "    for j in range(i + 1, len(unique_PPI)):\n",
    "        expt_1 = unique_PPI[i]\n",
    "        expt_2 = unique_PPI[j]\n",
    "        genes_1 = [protein_gene[unique_PPI[i][0]], protein_gene[unique_PPI[i][1]]]\n",
    "        genes_2 = [protein_gene[unique_PPI[j][0]], protein_gene[unique_PPI[j][1]]]\n",
    "        if(len([x for x in genes_1 if x in genes_2])):\n",
    "            edges.append(str(i) + \"\\t\" + str(j) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./utils/link_MS', 'w') as f:\n",
    "    f.writelines(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./utils/node_text', 'r') as f:\n",
    "#     lines_node_text = f.readlines()\n",
    "    \n",
    "# f.close()\n",
    "\n",
    "# with open('./utils/node_MS', 'r') as f:\n",
    "#     lines_node_ms = f.readlines()\n",
    "    \n",
    "# f.close()\n",
    "\n",
    "# lines = lines_node_ms + lines_node_text\n",
    "    \n",
    "# with open('node', 'w') as f:\n",
    "#     f.writelines(lines)\n",
    "\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./utils/link_text', 'r') as f:\n",
    "#     lines_link_text = f.readlines()\n",
    "    \n",
    "# f.close()\n",
    "\n",
    "# with open('./utils/link_MS', 'r') as f:\n",
    "#     lines_link_ms = f.readlines()\n",
    "    \n",
    "# f.close()\n",
    "\n",
    "# with open('link', 'w') as f:\n",
    "#     f.writelines(lines_link_ms)\n",
    "#     f.writelines(lines_link_text)\n",
    "\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
