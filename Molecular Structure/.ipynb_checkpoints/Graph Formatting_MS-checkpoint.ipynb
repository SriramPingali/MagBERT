{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program extracting first column \n",
    "import xlrd\n",
    "\n",
    "loc = (\"HPRD50_multimodal_dataset.xlsx\")\n",
    "\n",
    "wb = xlrd.open_workbook(loc)\n",
    "sheet = wb.sheet_by_index(1) \n",
    "sheet.cell_value(0, 0) \n",
    "PPI = []\n",
    "\n",
    "for i in range(sheet.nrows): \n",
    "    PPI.append([sheet.cell_value(i, 0), sheet.cell_value(i, 1), sheet.cell_value(i, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_PPI = [list(x) for x in set(tuple(x) for x in PPI)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract PDB Files from Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdb import *\n",
    "import Bio\n",
    "from Bio.PDB import PDBList\n",
    "pdbl = PDBList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_gene = {}\n",
    "def gene_to_pdb(pdb_id):\n",
    "    found_pdbs = Query(pdb_id).search()\n",
    "    print (found_pdbs)\n",
    "    temp= []\n",
    "    for j in found_pdbs:\n",
    "        temp.append(int(describe_pdb(j)['nr_residues']))\n",
    "    final_pdb_id = found_pdbs[temp.index(max(temp))]\n",
    "    protein_gene[pdb_id] = final_pdb_id\n",
    "    print (final_pdb_id)\n",
    "    pdbl.retrieve_pdb_file(final_pdb_id,pdir='PDB', file_format='pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_protein = []\n",
    "for proteins in unique_PPI:\n",
    "    unique_protein.append(proteins[0])\n",
    "    unique_protein.append(proteins[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 instances of unique proteins found from 769 documents\n"
     ]
    }
   ],
   "source": [
    "unique_protein = list(set(unique_protein))\n",
    "print(len(unique_protein), \"instances of unique proteins found from\", len(unique_PPI), \"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_PPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ppi in unique_protein:\n",
    "#     gene_to_pdb(ppi)\n",
    "#     if len(protein_gene) % 50 == 0:\n",
    "#         with open('./utils/protein_gene.txt', 'w') as f:\n",
    "#             f.write(str(protein_gene))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "with open('./utils/protein_gene.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "    protein_gene = ast.literal_eval(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit([\"N\", \"C\", \"O\", \"H\", \"S\", \"N1+\"])\n",
    "\n",
    "def get_feature_vector(gene):\n",
    "    temp = []\n",
    "    with open('./PDB/pdb' + gene.lower() +'.ent', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            split_ = line.split()\n",
    "            if split_[0] == 'ATOM':\n",
    "                temp.extend([float(split_[6]), float(split_[7]), float(split_[8])])  \n",
    "                temp.extend(label_encoder.transform([split_[-1]]))#.toarray().squeeze().tolist())\n",
    "    return(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6564a5bd429b46fc825010115fa076ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=769), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "nodes = []\n",
    "labels = []\n",
    "remove = []\n",
    "feature_vector = {}\n",
    "\n",
    "for i in tqdm_notebook(range(len(unique_PPI))):\n",
    "    proteins = unique_PPI[i]\n",
    "    try:\n",
    "        temp = []\n",
    "        gene_1 = protein_gene[proteins[0]]\n",
    "        gene_2 = protein_gene[proteins[1]]\n",
    "        if proteins[0] not in feature_vector:\n",
    "            feature_vector[gene_1] = get_feature_vector(gene_1)\n",
    "#         length.append(len(feature_vector[gene_1]))\n",
    "        temp.extend(feature_vector[gene_1])\n",
    "        temp.extend([0] * (431520 - len(temp)))\n",
    "        if proteins[1] not in feature_vector:\n",
    "            feature_vector[gene_2] = get_feature_vector(gene_2)\n",
    "#         length.append(len(feature_vector[gene_2]))\n",
    "        temp.extend(feature_vector[gene_2])\n",
    "        temp.extend([0] * (863040 - len(temp)))\n",
    "        nodes.append(temp)\n",
    "        labels.append(float(proteins[2]))\n",
    "    \n",
    "    except:\n",
    "        remove.append(proteins)\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min([len(node) for node in nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_PPI = [x for x in unique_PPI if x not in remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./utils/unique_ppi.txt', 'w') as filehandle:\n",
    "    for listitem in unique_PPI:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"./utils/node.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(nodes)\n",
    "\n",
    "with open(\"./utils/label.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "nodes = []\n",
    "\n",
    "with open('./utils/node.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if(len(row) != 0):\n",
    "            nodes.append([float(val) for val in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "with open('./utils/label.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        labels.extend([float(val) for val in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(nodes, labels, test_size=0, random_state=42)\n",
    "X_train = nodes\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(tens):\n",
    "    tens = torch.reshape(tens, (tens.shape[0], -1, 4))\n",
    "    tens = torch.transpose(tens, 1, 2)\n",
    "    return(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# X_train, y_train, X_test, y_test = map(torch.tensor, (X_train, y_train, X_test, y_test))\n",
    "# X_train, X_test = map(pre_process, (X_train, X_test))\n",
    "X_train, y_train = map(torch.tensor, (X_train, y_train))\n",
    "X_train = pre_process(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "#             MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "#             MaxPool2d(kernel_size=2, stride=2),\n",
    "            Conv2d(4, 4, kernel_size=5, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(431516, 1000)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "encoder = Net()\n",
    "\n",
    "nodes_enc = []\n",
    "for i in tqdm_notebook(range(len(X_train))):\n",
    "    encoding = [i]\n",
    "    encoding.extend(encoder.forward(X_train[i].unsqueeze(0).unsqueeze(0)).squeeze().tolist())\n",
    "    encoding.append(y_train[i].numpy())\n",
    "    nodes_enc.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./utils/node_enc.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(nodes_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for node in nodes_enc:\n",
    "    temp = \"\\t\".join([str(elem) for elem in node])\n",
    "    lines.append(temp + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./utils/node_MS', 'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define empty list\n",
    "unique_PPI = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open('./utils/unique_ppi.txt', 'r') as filehandle:\n",
    "    filecontents = filehandle.readlines()\n",
    "\n",
    "    for line in filecontents:\n",
    "        temp = []\n",
    "        # remove linebreak which is the last character of the string\n",
    "        line = line[:-1]\n",
    "        line = line.strip('[').strip(']').split(\", \")\n",
    "        temp = [item.replace(\"'\", \"\") for item in line]\n",
    "        # add item to the list\n",
    "        unique_PPI.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_PPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "with open('./utils/protein_gene.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "    protein_gene = ast.literal_eval(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for i in tqdm_notebook(range(0, len(unique_PPI))):\n",
    "    for j in range(i + 1, len(unique_PPI)):\n",
    "        expt_1 = unique_PPI[i]\n",
    "        expt_2 = unique_PPI[j]\n",
    "        genes_1 = [protein_gene[unique_PPI[i][0]], protein_gene[unique_PPI[i][1]]]\n",
    "        genes_2 = [protein_gene[unique_PPI[j][0]], protein_gene[unique_PPI[j][1]]]\n",
    "        if(len([x for x in genes_1 if x in genes_2])):\n",
    "            edges.append(str(i) + \"\\t\" + str(j) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./utils/link_MS', 'w') as f:\n",
    "    f.writelines(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./utils/node_text', 'r') as f:\n",
    "    lines_node_text = f.readlines()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "with open('./utils/node_MS', 'r') as f:\n",
    "    lines_node_ms = f.readlines()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "lines = lines_node_ms + lines_node_text\n",
    "    \n",
    "with open('../Graph-Bert/data/ppi/node', 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./utils/link_text', 'r') as f:\n",
    "    lines_link_text = f.readlines()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "with open('./utils/link_MS', 'r') as f:\n",
    "    lines_link_ms = f.readlines()\n",
    "    \n",
    "f.close()\n",
    "\n",
    "with open('../Graph-Bert/data/ppi/link', 'w') as f:\n",
    "    f.writelines(lines_link_ms)\n",
    "    f.writelines(lines_link_text)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
